{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtl3b-lNEhP9"
      },
      "outputs": [],
      "source": [
        "pip install modAL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/modAL-python/modAL.git"
      ],
      "metadata": {
        "id": "HRnZwVrkEqR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, RBF, ExpSineSquared\n",
        "from modAL.models import BayesianOptimizer\n",
        "from modAL.acquisition import optimizer_EI, max_EI, max_UCB, max_PI\n",
        "from ipywidgets.widgets import interact,interactive\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "plt.rcParams[\"figure.figsize\"] = (15,8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1sF6VDxhE1B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seaborn"
      ],
      "metadata": {
        "id": "9I846Wp6E_bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kGU5ifkbFRTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Connect to Google Sheets\n",
        "# ================================\n",
        "!pip install --quiet gspread gspread_dataframe\n",
        "\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from gspread_dataframe import get_as_dataframe"
      ],
      "metadata": {
        "id": "cZW4_RNg0l0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "mzO_sFjU2pn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ‘‰ Replace with your Google Sheets URL or ID\n",
        "SHEET_URL = '' #insert relevant link\n",
        "# Extract the spreadsheet ID from the URL\n",
        "spreadsheet_id = SHEET_URL.split('/')[-2]\n",
        "spreadsheet = gc.open_by_key(spreadsheet_id)\n",
        "worksheet = spreadsheet.worksheet(\"Sheet1\")\n",
        "\n",
        "# Convert to pandas DataFrame using get_as_dataframe and specifying the header row\n",
        "data = get_as_dataframe(\n",
        "    worksheet,\n",
        "    usecols=[1,2,3,4,5,6,7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], # keep cols B-U.\n",
        "    header=1)\n",
        "\n",
        "\n",
        "print(\"Data loaded from Google Sheets:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRtJ7adofM-d",
        "outputId": "5dbf7649-5d54-4777-b093-9e9a3afc1010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded from Google Sheets:\n",
            "   Fraction_Fe  Fraction_Al  Fraction_Si  s_loc_Fe  s_scale_Fe  s_mode_Fe  \\\n",
            "0     0.002043     0.790271     0.207685  4.667805    2.629459   1.000000   \n",
            "1     0.001644     0.793107     0.205249  4.312864    3.225134   0.999983   \n",
            "2     0.003751     0.797295     0.198954  1.968683    5.804383   1.000000   \n",
            "3     0.002646     0.715634     0.281720  3.578333    2.704356   1.000000   \n",
            "4     0.003938     0.713604     0.282458  1.443828    5.353665   0.999999   \n",
            "\n",
            "   s_loc_Al  s_scale_Al  s_mode_Al  s_loc_Si  s_scale_Si  s_mode_Si  \\\n",
            "0  4.723456   13.701967   0.397869  0.897714    5.245641   0.271674   \n",
            "1  7.354702    6.462756   0.439239  0.684782    5.327519   0.338023   \n",
            "2  0.642170    5.767209   7.097608  0.764883    4.997357   0.336441   \n",
            "3  6.296788    4.418780   0.902652  0.474849    3.900660   0.274855   \n",
            "4  6.137379    4.690521   0.881376  0.514242    3.937513   0.255552   \n",
            "\n",
            "     angle_Fe    angle_Al    angle_Si     AR_Fe     AR_Al     AR_Si  UTS/MPa  \\\n",
            "0  244.423251  181.910736  182.191875  0.152705  0.867027  0.769129   277.52   \n",
            "1  237.948710  180.265028  182.231228  0.165927  0.165927  0.739643   284.97   \n",
            "2  160.200574  180.832398  182.185924  0.178366  0.859206  0.750673   263.31   \n",
            "3  165.329277  180.838333  179.658620  0.077591  0.876815  0.690354   274.15   \n",
            "4  228.589190  179.092331  180.040350  0.104767  0.877032  0.695337   267.56   \n",
            "\n",
            "      G N/m  \n",
            "0  268.7591  \n",
            "1  280.6735  \n",
            "2  273.0376  \n",
            "3  261.3481  \n",
            "4  254.0653  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 2. Prepare the data\n",
        "# ================================\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
        "\n",
        "# Select the relevant rows first\n",
        "\n",
        "# Expecting columns named \"X\", \"Y\", \"Z\". Reshape for consistency with your script.\n",
        "X1 = data[\"Fraction_Fe\"].to_numpy().reshape(-1,1)\n",
        "X2 = data[\"Fraction_Al\"].to_numpy().reshape(-1,1)\n",
        "X3 = data[\"Fraction_Si\"].to_numpy().reshape(-1,1)\n",
        "X4 = data[\"s_loc_Fe\"].to_numpy().reshape(-1,1)\n",
        "X5 = data[\"s_scale_Fe\"].to_numpy().reshape(-1,1)\n",
        "X6 = data[\"s_mode_Fe\"].to_numpy().reshape(-1,1)\n",
        "X7 = data[\"s_loc_Al\"].to_numpy().reshape(-1,1)\n",
        "X8 = data[\"s_scale_Al\"].to_numpy().reshape(-1,1)\n",
        "X9 = data[\"s_mode_Al\"].to_numpy().reshape(-1,1)\n",
        "X10 = data[\"s_loc_Si\"].to_numpy().reshape(-1,1)\n",
        "X11 = data[\"s_scale_Si\"].to_numpy().reshape(-1,1)\n",
        "X12 = data[\"s_mode_Si\"].to_numpy().reshape(-1,1)\n",
        "X13 = data[\"angle_Fe\"].to_numpy().reshape(-1,1)\n",
        "X14 = data[\"angle_Al\"].to_numpy().reshape(-1,1)\n",
        "X15 = data[\"angle_Si\"].to_numpy().reshape(-1,1)\n",
        "X16 = data[\"AR_Fe\"].to_numpy().reshape(-1,1)\n",
        "X17 = data[\"AR_Al\"].to_numpy().reshape(-1,1)\n",
        "X18 = data[\"AR_Si\"].to_numpy().reshape(-1,1)\n",
        "#Z = data[\"UTS/MPa\"].to_numpy().reshape(-1,1)\n",
        "Z = data[\"G N/m\"].to_numpy().reshape(-1,1)\n",
        "\n",
        "\n",
        "n_features = 18\n",
        "\n",
        "# Stack features\n",
        "X_s = np.hstack((X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15,X16,X17,X18))\n",
        "\n",
        "\n",
        "print(\"Shape of X_s:\", np.shape(X_s))\n",
        "\n",
        "# Scaling\n",
        "SS_x = StandardScaler()\n",
        "SS_y = StandardScaler()\n",
        "\n",
        "X_t = SS_x.fit_transform(X_s)\n",
        "y = SS_y.fit_transform(Z.reshape(-1, 1))\n",
        "print(\"Shape of y:\", np.shape(y))\n",
        "\n",
        "# Latin Hypercube Sampling for initial training set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_initial_points = 10\n",
        "\n",
        "# Using KMeans to select initial points that are spread out in the feature space\n",
        "kmeans = KMeans(n_clusters=n_initial_points, random_state=0, n_init=10)\n",
        "kmeans.fit(X_t)\n",
        "initial_indices = []\n",
        "for i in range(n_initial_points):\n",
        "    # Find the index of the point closest to each cluster center\n",
        "    distances = np.linalg.norm(X_t - kmeans.cluster_centers_[i], axis=1)\n",
        "    closest_index = np.argmin(distances)\n",
        "    initial_indices.append(closest_index)\n",
        "\n",
        "\n",
        "X_initial, y_initial = X_t[initial_indices], y[initial_indices]\n",
        "print(f\"Selected {n_initial_points} initial points using KMeans-based sampling.\")\n",
        "\n",
        "\n",
        "print(initial_indices)\n",
        "print(\"Shape of X_initial:\", np.shape(X_initial))\n",
        "print(\"Shape of y_initial:\", np.shape(y_initial))\n",
        "\n",
        "# ================================\n",
        "# 3. Define the model & optimizer\n",
        "# ================================\n",
        "\n",
        "kernel = Matern(length_scale=[1.0]*n_features, length_scale_bounds=(1e-1, 10.0), nu=1.5) + WhiteKernel(noise_level=1)\n",
        "#kernel = Matern(length_scale=[1.0]*n_features, length_scale_bounds=(1e-1, 10.0), nu=0.5)\n",
        "#kernel = WhiteKernel(noise_level=1)\n",
        "regressor = GaussianProcessRegressor(kernel=kernel,\n",
        "                     alpha               = 5e-10,\n",
        "                     copy_X_train        = False,\n",
        "                     optimizer           = \"fmin_l_bfgs_b\",\n",
        "                     n_restarts_optimizer= 0,\n",
        "                     normalize_y         = False,\n",
        "                     random_state        = None)\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimizer(\n",
        "    estimator=regressor,\n",
        "    X_training=X_initial,\n",
        "    y_training=y_initial,\n",
        "    query_strategy=max_EI\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 4. Run Bayesian Optimization\n",
        "# ================================\n",
        "query_list, x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12, x_13, x_14, x_15, x_16, x_17, x_18, z_ = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "\n",
        "for n_query in range(50):\n",
        "    query_idx, query_inst = optimizer.query(X_t)\n",
        "    query_list.append(query_idx)\n",
        "    optimizer.teach(X_t[query_idx,:].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "\n",
        "    y_pred, y_std = optimizer.predict(X_t, return_std=True)\n",
        "    y_pred, y_std = y_pred.ravel(), y_std.ravel()\n",
        "    X_max, y_max = optimizer.get_max()\n",
        "\n",
        "    x1 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][0]\n",
        "    x2 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][1]\n",
        "    x3 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][2]\n",
        "    x4 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][3]\n",
        "    x5 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][4]\n",
        "    x6 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][5]\n",
        "    x7 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][6]\n",
        "    x8 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][7]\n",
        "    x9 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][8]\n",
        "    x10 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][9]\n",
        "    x11 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][10]\n",
        "    x12 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][11]\n",
        "    x13 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][12]\n",
        "    x14 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][13]\n",
        "    x15 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][14]\n",
        "    x16 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][15]\n",
        "    x17 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][16]\n",
        "    x18 = SS_x.inverse_transform(X_t[query_idx,:].reshape(1, -1))[0][17]\n",
        "    z  = SS_y.inverse_transform(y[query_idx][0].reshape(-1, 1)).item()\n",
        "\n",
        "\n",
        "    x_1.append(x1)\n",
        "    x_2.append(x2)\n",
        "    x_3.append(x3)\n",
        "    x_4.append(x4)\n",
        "    x_5.append(x5)\n",
        "    x_6.append(x6)\n",
        "    x_7.append(x7)\n",
        "    x_8.append(x8)\n",
        "    x_9.append(x9)\n",
        "    x_10.append(x10)\n",
        "    x_11.append(x11)\n",
        "    x_12.append(x12)\n",
        "    x_13.append(x13)\n",
        "    x_14.append(x14)\n",
        "    x_15.append(x15)\n",
        "    x_16.append(x16)\n",
        "    x_17.append(x17)\n",
        "    x_18.append(x18)\n",
        "    z_.append(z)\n",
        "\n",
        "X_max_final_scaled, y_max_in_queried_final_scaled = optimizer.get_max()\n",
        "X_max_pred_final = SS_x.inverse_transform(X_max_final_scaled.reshape(1, -1))\n",
        "y_max_data_final = SS_y.inverse_transform(y_max_in_queried_final_scaled.reshape(1, -1))\n",
        "\n",
        "print(\"Best point after\", len(optimizer.y_training) - n_initial_points, \"queries is:\", X_max_pred_final, y_max_data_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "claNCFYEWPCh",
        "outputId": "f90885fb-2578-4ea8-a651-463805a83fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_s: (225, 18)\n",
            "Shape of y: (225, 1)\n",
            "Selected 10 initial points using KMeans-based sampling.\n",
            "[np.int64(107), np.int64(68), np.int64(144), np.int64(223), np.int64(1), np.int64(184), np.int64(81), np.int64(19), np.int64(53), np.int64(2)]\n",
            "Shape of X_initial: (10, 18)\n",
            "Shape of y_initial: (10, 1)\n",
            "Best point after 50 queries is: [[2.70100000e-03 7.93180000e-01 2.04119000e-01 3.72159000e+00\n",
            "  4.87635500e+00 2.90585900e-06 6.42370000e+00 7.01377500e+00\n",
            "  5.49965000e-01 7.80296000e-01 5.23962600e+00 3.00972000e-01\n",
            "  8.92107980e+01 1.83667817e+02 1.83442550e+02 1.10007000e-01\n",
            "  8.59163000e-01 7.48413000e-01]] [[415.9884]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDiyA34cY_hE"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ================================\n",
        "# 5. Report best result\n",
        "# ================================\n",
        "X_max_final_scaled, y_max_in_queried_final_scaled = optimizer.get_max()\n",
        "X_max_pred_final = SS_x.inverse_transform(X_max_final_scaled.reshape(1, -1))\n",
        "y_max_data_final = SS_y.inverse_transform(y_max_in_queried_final_scaled.reshape(1, -1))\n",
        "\n",
        "print(\"Best point after\", len(optimizer.y_training) - n_initial_points, \"queries is:\", X_max_pred_final, y_max_data_final)\n",
        "\n",
        "# Get the standard deviation at the best point\n",
        "_, y_std_at_max_scaled = optimizer.predict(X_max_final_scaled.reshape(1, -1), return_std=True)\n",
        "# Note: Standard deviation is not scaled in the same way as the mean,\n",
        "# but the value directly from the GP model is on the scaled target space.\n",
        "# To interpret it on the original scale, you can multiply by the standard deviation of the original target data.\n",
        "y_std_at_max_original_scale = y_std_at_max_scaled.item() * SS_y.scale_[0]\n",
        "\n",
        "\n",
        "print(f\"Predicted standard deviation at the best point (on original scale): {y_std_at_max_original_scale:.4f}\")\n",
        "\n",
        "\n",
        "#ax.scatter(x1, x2, x3, z, c='k', s=50)\n",
        "#ax.text(x1, x2, z, f\"No. {n_query}\", zdir=(0, 0, 0))\n",
        "\n",
        "#ax.set_xlabel(\"X\")\n",
        "#ax.set_ylabel(\"Y\")\n",
        "#ax.set_zlabel(r\"f(x,y)\")\n",
        "#ax.plot(x_1, x_2, z_, 'r-->', alpha=0.8, linewidth=2)\n",
        "\n",
        "# Calculate and store the best value found at each iteration\n",
        "best_values_at_each_iter = [SS_y.inverse_transform(optimizer.y_training[:i].max().reshape(1, -1)).item() for i in range(1, len(optimizer.y_training) + 1)]\n",
        "\n",
        "# Get the values obtained at each query\n",
        "queried_values = [SS_y.inverse_transform(y_val.reshape(1, -1)).item() for y_val in optimizer.y_training]\n",
        "\n",
        "# The plotting of convergence, simple regret, cumulative regret, and MAPE\n",
        "# should ideally be done after the optimization loop is complete in HLhol4cO3A3d\n",
        "# or in separate cells that use the collected data lists.\n",
        "# The code below is moved or should be placed after the optimization loop.\n",
        "\n",
        "print(\"\\nBest value found and queried value at each iteration:\")\n",
        "for i, (best_val, queried_val) in enumerate(zip(best_values_at_each_iter, queried_values)):\n",
        "    print(f\"Iteration {i+1}: Best Value = {best_val:.4f}, Queried Value = {queried_val:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(best_values_at_each_iter) + 1), best_values_at_each_iter, marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Best Objective Function Value Found')\n",
        "plt.title('Convergence of Bayesian Optimization (Best Value vs. Iterations)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 6. Report performance metrics\n",
        "# ================================\n",
        "\n",
        "# Calculate the true maximum value in the dataset\n",
        "max_true_value = SS_y.inverse_transform(y.max().reshape(1, -1)).item()\n",
        "\n",
        "# Calculate the simple regret at each iteration\n",
        "simple_regret_at_each_iter = [max_true_value - best_val for best_val in best_values_at_each_iter]\n",
        "\n",
        "# Plot the simple regret at each iteration\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(simple_regret_at_each_iter) + 1), simple_regret_at_each_iter, marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Simple Regret')\n",
        "plt.title('Bayesian Optimization Simple Regret vs. Iterations')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate the cumulative regret at each iteration (sum of simple regrets - not standard definition of cumulative regret)\n",
        "cumulative_regret_at_each_iter = np.cumsum(simple_regret_at_each_iter)\n",
        "\n",
        "# Calculate INSTANTANEOUS REGRET\n",
        "instantaneous_regret_at_each_iter = [max_true_value - queried_val for queried_val in queried_values]\n",
        "# Calculate actual cumulative regret\n",
        "cumul_regret_at_each_iter = np.cumsum(instantaneous_regret_at_each_iter)\n",
        "\n",
        "# Plot the cumulative regret\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumul_regret_at_each_iter) + 1), cumul_regret_at_each_iter, marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Cumulative Regret')\n",
        "plt.title('Bayesian Optimization Cumulative Regret vs. Iterations')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Mean Absolute Percentage Error (MAPE)\n",
        "# MAPE = (1/n) * sum(|(Actual - Forecast) / Actual|) * 100\n",
        "# In this context, \"Actual\" can be considered the best value found so far,\n",
        "# and \"Forecast\" is the queried value at each iteration.\n",
        "\n",
        "# Ensure both lists have the same length\n",
        "if len(best_values_at_each_iter) != len(queried_values):\n",
        "     print(\"Error: The length of best_values_at_each_iter and queried_values do not match.\")\n",
        "else:\n",
        "    # Calculate absolute percentage error for each iteration\n",
        "    absolute_percentage_errors = [\n",
        "        np.abs((best - queried) / best) * 100\n",
        "        for best, queried in zip(best_values_at_each_iter, queried_values)\n",
        "        if best != 0  # Avoid division by zero if best value is 0\n",
        "    ]\n",
        "\n",
        "    # Calculate the cumulative mean of the absolute percentage errors\n",
        "    cumulative_mean_absolute_percentage_error = np.cumsum(absolute_percentage_errors) / np.arange(1, len(absolute_percentage_errors) + 1)\n",
        "\n",
        "\n",
        "    # Calculate the final mean of the absolute percentage errors\n",
        "    if absolute_percentage_errors:\n",
        "        mape = np.mean(absolute_percentage_errors)\n",
        "        print(f\"\\nMean Absolute Percentage Error (MAPE): {mape:.4f}%\")\n",
        "\n",
        "        # Plot the cumulative mean absolute percentage error at each iteration\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(cumulative_mean_absolute_percentage_error) + 1), cumulative_mean_absolute_percentage_error, marker='o', linestyle='-')\n",
        "        plt.xlabel('Number of Queries')\n",
        "        plt.ylabel('Cumulative Mean Absolute Percentage Error (%)')\n",
        "        plt.title('Cumulative Mean Absolute Percentage Error vs. Iterations')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"\\nCould not calculate or plot MAPE (possibly due to best values being zero).\")\n",
        "\n",
        "\n",
        "# Predict the objective function values for all the scaled input data\n",
        "y_pred_scaled, _ = optimizer.predict(X_t, return_std=True)\n",
        "\n",
        "# Inverse transform the predicted values and the true values back to their original scale\n",
        "y_pred = SS_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "y_true = SS_y.inverse_transform(y).ravel()\n",
        "\n",
        "# Create a scatter plot of true vs. predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_true, y_pred, alpha=0.5)\n",
        "\n",
        "# Add a diagonal line\n",
        "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2)\n",
        "\n",
        "# Label axes and add title\n",
        "plt.xlabel('True Effective Fracture Energy N/m')\n",
        "plt.ylabel('Predicted Effective Fracture Energy N/m')\n",
        "plt.title('Predicted vs. True Objective Function Values')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "plt.text(0.05, 0.95, f\"RÂ² = {r2:.3f}\\nRMSE = {rmse:.3f}\", transform=plt.gca().transAxes,\n",
        "         fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "\n",
        "y_pred_scaled, y_std_scaled = optimizer.predict(X_t, return_std=True)\n",
        "plt.scatter(y_true, y_pred, c=y_std_scaled, cmap='viridis', alpha=0.7)\n",
        "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2) # Add a diagonal line\n",
        "plt.colorbar(label='Prediction uncertainty')\n",
        "plt.xlabel('True Effective Fracture Energy N/m')\n",
        "plt.ylabel('Predicted Effective Fracture Energy N/m')\n",
        "plt.title('Predicted vs. True Objective Function Values')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 7. Save Results to Google Drive\n",
        "# ================================\n",
        "\n",
        "# Define the directory for saving results\n",
        "# You can change this directory name\n",
        "save_directory = '/content/drive/MyDrive/Bayesian_Optimization_Results'\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "print(f\"\\nSaving results to: {save_directory}\")\n",
        "\n",
        "# Save best point and standard deviation\n",
        "best_point_data = {\n",
        "    'Best X (Original Scale)': [X_max_pred_final[0].tolist()], # Convert numpy array to list of list for CSV\n",
        "    'Best Y (Original Scale)': [y_max_data_final[0].tolist()], # Convert numpy array to list of list for CSV\n",
        "    'Predicted Std Dev at Best Point (Original Scale)': [y_std_at_max_original_scale]\n",
        "}\n",
        "best_point_df = pd.DataFrame(best_point_data)\n",
        "best_point_df.to_csv(os.path.join(save_directory, 'best_point_results.csv'), index=False)\n",
        "print(f\"Best point results saved to: {os.path.join(save_directory, 'best_point_results.csv')}\")\n",
        "\n",
        "\n",
        "# Save best value found and queried value at each iteration\n",
        "iteration_values_data = {\n",
        "    'Iteration': range(1, len(best_values_at_each_iter) + 1),\n",
        "    'Best Value Found': best_values_at_each_iter,\n",
        "    'Queried Value': queried_values\n",
        "}\n",
        "iteration_values_df = pd.DataFrame(iteration_values_data)\n",
        "iteration_values_df.to_csv(os.path.join(save_directory, 'iteration_values.csv'), index=False)\n",
        "print(f\"Iteration values saved to: {os.path.join(save_directory, 'iteration_values.csv')}\")\n",
        "\n",
        "\n",
        "# Save data and plot for Convergence\n",
        "convergence_data = {\n",
        "    'Number of Queries': range(1, len(best_values_at_each_iter) + 1),\n",
        "    'Best Objective Function Value Found': best_values_at_each_iter\n",
        "}\n",
        "convergence_df = pd.DataFrame(convergence_data)\n",
        "convergence_df.to_csv(os.path.join(save_directory, 'convergence_data.csv'), index=False)\n",
        "print(f\"Convergence data saved to: {os.path.join(save_directory, 'convergence_data.csv')}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(convergence_df['Number of Queries'], convergence_df['Best Objective Function Value Found'], marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Best Objective Function Value Found')\n",
        "plt.title('Convergence of Bayesian Optimization (Best Value vs. Iterations)')\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(save_directory, 'convergence_plot.png'))\n",
        "plt.close() # Close the plot to free memory\n",
        "print(f\"Convergence plot saved to: {os.path.join(save_directory, 'convergence_plot.png')}\")\n",
        "\n",
        "# Save data and plot for Simple Regret\n",
        "simple_regret_data = {\n",
        "    'Number of Queries': range(1, len(simple_regret_at_each_iter) + 1),\n",
        "    'Simple Regret': simple_regret_at_each_iter\n",
        "}\n",
        "simple_regret_df = pd.DataFrame(simple_regret_data)\n",
        "simple_regret_df.to_csv(os.path.join(save_directory, 'simple_regret_data.csv'), index=False)\n",
        "print(f\"Simple regret data saved to: {os.path.join(save_directory, 'simple_regret_data.csv')}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(simple_regret_df['Number of Queries'], simple_regret_df['Simple Regret'], marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Simple Regret')\n",
        "plt.title('Bayesian Optimization Simple Regret vs. Iterations')\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(save_directory, 'simple_regret_plot.png'))\n",
        "plt.close()\n",
        "print(f\"Simple regret plot saved to: {os.path.join(save_directory, 'simple_regret_plot.png')}\")\n",
        "\n",
        "# Save data and plot for Cumulative Regret (based on simple regret sum)\n",
        "cumulative_regret_simple_sum_data = {\n",
        "    'Number of Queries': range(1, len(cumulative_regret_at_each_iter) + 1),\n",
        "    'Cumulative Regret (Simple Sum)': cumulative_regret_at_each_iter\n",
        "}\n",
        "cumulative_regret_simple_sum_df = pd.DataFrame(cumulative_regret_simple_sum_data)\n",
        "cumulative_regret_simple_sum_df.to_csv(os.path.join(save_directory, 'cumulative_regret_simple_sum_data.csv'), index=False)\n",
        "print(f\"Cumulative regret (simple sum) data saved to: {os.path.join(save_directory, 'cumulative_regret_simple_sum_data.csv')}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_regret_simple_sum_df['Number of Queries'], cumulative_regret_simple_sum_df['Cumulative Regret (Simple Sum)'], marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Cumulative Regret (Simple Sum)')\n",
        "plt.title('Bayesian Optimization Cumulative Regret (Simple Sum) vs. Iterations')\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(save_directory, 'cumulative_regret_simple_sum_plot.png'))\n",
        "plt.close()\n",
        "print(f\"Cumulative regret (simple sum) plot saved to: {os.path.join(save_directory, 'cumulative_regret_simple_sum_plot.png')}\")\n",
        "\n",
        "# Save data and plot for Cumulative Regret (based on instantaneous regret sum)\n",
        "cumulative_regret_instantaneous_data = {\n",
        "    'Number of Queries': range(1, len(cumul_regret_at_each_iter) + 1),\n",
        "    'Cumulative Regret (Instantaneous Sum)': cumul_regret_at_each_iter\n",
        "}\n",
        "cumulative_regret_instantaneous_df = pd.DataFrame(cumulative_regret_instantaneous_data)\n",
        "cumulative_regret_instantaneous_df.to_csv(os.path.join(save_directory, 'cumulative_regret_instantaneous_data.csv'), index=False)\n",
        "print(f\"Cumulative regret (instantaneous sum) data saved to: {os.path.join(save_directory, 'cumulative_regret_instantaneous_data.csv')}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_regret_instantaneous_df['Number of Queries'], cumulative_regret_instantaneous_df['Cumulative Regret (Instantaneous Sum)'], marker='o', linestyle='-')\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Cumulative Regret')\n",
        "plt.title('Bayesian Optimization Cumulative Regret vs. Iterations')\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(save_directory, 'cumulative_regret_instantaneous_plot.png'))\n",
        "plt.close()\n",
        "print(f\"Cumulative regret (instantaneous sum) plot saved to: {os.path.join(save_directory, 'cumulative_regret_instantaneous_plot.png')}\")\n",
        "\n",
        "\n",
        "# Save data and plot for Absolute Percentage Error and MAPE\n",
        "if absolute_percentage_errors:\n",
        "    absolute_percentage_error_data = {\n",
        "        'Number of Queries': range(1, len(absolute_percentage_errors) + 1),\n",
        "        'Absolute Percentage Error (%)': absolute_percentage_errors\n",
        "    }\n",
        "    absolute_percentage_error_df = pd.DataFrame(absolute_percentage_error_data)\n",
        "    absolute_percentage_error_df.to_csv(os.path.join(save_directory, 'absolute_percentage_error_data.csv'), index=False)\n",
        "    print(f\"Absolute percentage error data saved to: {os.path.join(save_directory, 'absolute_percentage_error_data.csv')}\")\n",
        "\n",
        "    mape_data = {'Mean Absolute Percentage Error (MAPE)': [mape]}\n",
        "    mape_df = pd.DataFrame(mape_data)\n",
        "    mape_df.to_csv(os.path.join(save_directory, 'mape_data.csv'), index=False)\n",
        "    print(f\"MAPE data saved to: {os.path.join(save_directory, 'mape_data.csv')}\")\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(absolute_percentage_error_df) + 1), absolute_percentage_error_df['Absolute Percentage Error (%)'], marker='o', linestyle='-')\n",
        "    plt.xlabel('Number of Queries')\n",
        "    plt.ylabel('Absolute Percentage Error (%)')\n",
        "    plt.title('Absolute Percentage Error vs. Iterations')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_directory, 'absolute_percentage_error_plot.png'))\n",
        "    plt.close()\n",
        "    print(f\"Absolute percentage error plot saved to: {os.path.join(save_directory, 'absolute_percentage_error_plot.png')}\")\n",
        "\n",
        "    # Plot Cumulative Mean Absolute Percentage Error\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(cumulative_mean_absolute_percentage_error) + 1), cumulative_mean_absolute_percentage_error, marker='o', linestyle='-')\n",
        "    plt.xlabel('Number of Queries')\n",
        "    plt.ylabel('Cumulative Mean Absolute Percentage Error (%)')\n",
        "    plt.title('Cumulative Mean Absolute Percentage Error vs. Iterations')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_directory, 'cumulative_mean_absolute_percentage_error_plot.png'))\n",
        "    plt.close()\n",
        "    print(f\"Cumulative Mean Absolute Percentage Error plot saved to: {os.path.join(save_directory, 'cumulative_mean_absolute_percentage_error_plot.png')}\")\n",
        "\n",
        "\n",
        "# Save data and plot for Predicted vs True Objective Function Values\n",
        "predicted_vs_true_data = {\n",
        "    'True Effective Fracture Energy N/m': y_true,\n",
        "    'Predicted Effective Fracture Energy N/m': y_pred\n",
        "}\n",
        "predicted_vs_true_df = pd.DataFrame(predicted_vs_true_data)\n",
        "predicted_vs_true_df.to_csv(os.path.join(save_directory, 'predicted_vs_true_data.csv'), index=False)\n",
        "print(f\"Predicted vs true data saved to: {os.path.join(save_directory, 'predicted_vs_true_data.csv')}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(predicted_vs_true_df['True Effective Fracture Energy N/m'], predicted_vs_true_df['Predicted Effective Fracture Energy N/m'], alpha=0.5)\n",
        "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2) # Add diagonal line\n",
        "plt.xlabel('True Effective Fracture Energy N/m')\n",
        "plt.ylabel('Predicted Effective Fracture Energy N/m')\n",
        "plt.title('Predicted vs. True Objective Function Values')\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(save_directory, 'predicted_vs_true_plot.png'))\n",
        "plt.close()\n",
        "print(f\"Predicted vs true plot saved to: {os.path.join(save_directory, 'predicted_vs_true_plot.png')}\")\n",
        "\n",
        "print(\"\\nAll requested data and plots saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}